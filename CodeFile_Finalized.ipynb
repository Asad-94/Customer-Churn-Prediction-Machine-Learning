{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/1/1e/Institute_of_Business_Administration%2C_Karachi_%28logo%29.png/300px-Institute_of_Business_Administration%2C_Karachi_%28logo%29.png\" width=\"100px\">\n",
    "\n",
    "# **MACHINE LEARNING I - FINAL PROJECT**\n",
    "## **CUSTOMER CHURN PREDICTION**\n",
    "### Syed Asad Rizvi  ERP 25365\n",
    "### Fareed Hassan Khan  ERP 25367\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:25.684297Z",
     "iopub.status.busy": "2022-05-15T13:35:25.683931Z",
     "iopub.status.idle": "2022-05-15T13:35:33.017555Z",
     "shell.execute_reply": "2022-05-15T13:35:33.016838Z",
     "shell.execute_reply.started": "2022-05-15T13:35:25.684205Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from numpy import mean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from mixed_naive_bayes import MixedNB\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# For removing any warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:33.019366Z",
     "iopub.status.busy": "2022-05-15T13:35:33.018747Z",
     "iopub.status.idle": "2022-05-15T13:35:35.153718Z",
     "shell.execute_reply": "2022-05-15T13:35:35.152806Z",
     "shell.execute_reply.started": "2022-05-15T13:35:33.019328Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Telecom_customer churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the first few rows and the shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.155075Z",
     "iopub.status.busy": "2022-05-15T13:35:35.154874Z",
     "iopub.status.idle": "2022-05-15T13:35:35.163502Z",
     "shell.execute_reply": "2022-05-15T13:35:35.162688Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.155050Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cleaning** the dataset  (Removing NAN values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.165650Z",
     "iopub.status.busy": "2022-05-15T13:35:35.165388Z",
     "iopub.status.idle": "2022-05-15T13:35:35.464482Z",
     "shell.execute_reply": "2022-05-15T13:35:35.463440Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.165590Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['rev_Mean', 'kid11_15', 'dualband', 'area', 'hnd_price', 'change_mou'], inplace=True)\n",
    "df.drop(['avg6mou', 'avg6qty', 'avg6rev', 'prizm_social_one', 'ownrent', 'lor', 'dwlltype', 'adults', 'infobase', 'numbcars', \n",
    "'HHstatin', 'dwllsize', 'income', 'hnd_webcap'], axis=1, inplace=True)\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.466267Z",
     "iopub.status.busy": "2022-05-15T13:35:35.466051Z",
     "iopub.status.idle": "2022-05-15T13:35:35.472029Z",
     "shell.execute_reply": "2022-05-15T13:35:35.471210Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.466241Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One hot encoding** on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.473602Z",
     "iopub.status.busy": "2022-05-15T13:35:35.473366Z",
     "iopub.status.idle": "2022-05-15T13:35:35.733899Z",
     "shell.execute_reply": "2022-05-15T13:35:35.732957Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.473576Z"
    }
   },
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.736608Z",
     "iopub.status.busy": "2022-05-15T13:35:35.735922Z",
     "iopub.status.idle": "2022-05-15T13:35:35.781748Z",
     "shell.execute_reply": "2022-05-15T13:35:35.780963Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.736564Z"
    }
   },
   "outputs": [],
   "source": [
    "df_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fitting model code** for calculating the **ROC AUC** value of each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.783908Z",
     "iopub.status.busy": "2022-05-15T13:35:35.783661Z",
     "iopub.status.idle": "2022-05-15T13:35:35.789678Z",
     "shell.execute_reply": "2022-05-15T13:35:35.788820Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.783880Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(model, model_name):\n",
    "    model.fit(trainX,trainy)\n",
    "    md_probs = model.predict_proba(testX)\n",
    "    md_probs = md_probs[:,1]\n",
    "    md_auc = roc_auc_score(testy, md_probs)\n",
    "    print(model_name, \" : \", md_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Models**\n",
    "\n",
    "In the next step, several **machine learning classification** methods were applied on the **one-hot**\n",
    "encoded dataset one-by-one.\n",
    "These methods include:\n",
    "\n",
    "* K-Nearest Neighbors\n",
    "\n",
    "* Logistic Regression\n",
    "\n",
    "* Decision Trees\n",
    "\n",
    "* Random Forest\n",
    "\n",
    "* Gradient Boosting\n",
    "\n",
    "* Naïve Bayes \n",
    "  \n",
    "  - Gaussian NB\n",
    "\n",
    "  - Categorical NB\n",
    "  \n",
    "  - Mixed NB\n",
    "\n",
    "* Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df[['churn']]\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(df_onehot, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) K-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()),(\"KNN Classifier\", KNeighborsClassifier(n_neighbors=5))])\n",
    "fit_model(pipe_kn, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:35.791016Z",
     "iopub.status.busy": "2022-05-15T13:35:35.790804Z",
     "iopub.status.idle": "2022-05-15T13:35:36.016129Z",
     "shell.execute_reply": "2022-05-15T13:35:36.015157Z",
     "shell.execute_reply.started": "2022-05-15T13:35:35.790990Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lg = Pipeline([(\"scaler\", MinMaxScaler()),(\"Logistic\", LogisticRegression())])\n",
    "fit_model(pipe_lg, \"Logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5)  \n",
    "fit_model(dt, \"Decision Tree\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=20,n_estimators=1000)\n",
    "fit_model(rf, \"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including Learning rate parameter\n",
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200, learning_rate=0.05)\n",
    "fit_model(gb, \"Graident Boosting Classifier With Learning Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB\n",
    "nb_g = GaussianNB()\n",
    "fit_model(nb_g, \"Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical NB\n",
    "\n",
    "categorical_columns = list(df.columns[df.dtypes == 'object'])\n",
    "categorical_columns.append('churn')\n",
    "\n",
    "\n",
    "def convert_categorical(df1):\n",
    "    df_q = pd.DataFrame()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in df1:\n",
    "        if col not in categorical_columns:\n",
    "            df_q[col] = pd.qcut(df1[col], 5, duplicates='drop')            \n",
    "            df_q[col]= label_encoder.fit_transform(df_q[col])\n",
    "            df_q[col] = df_q[col].astype('str')\n",
    "\n",
    "    X_cat = df1[categorical_columns[:-1]]\n",
    "    df_cat = pd.concat([df_q,X_cat],axis=1)\n",
    "    return df_cat\n",
    "\n",
    " \n",
    "temp_df1 = convert_categorical(df) \n",
    "temp_df1.head()\n",
    "\n",
    "X_cat = convert_categorical(df)\n",
    "trainX, testX, trainy, testy = train_test_split(X_cat, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Model Code\n",
    "nb_c = CategoricalNB(min_categories = 100)\n",
    "fit_model(nb_c, \"Naive Bayes Categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed NB\n",
    "nb_mix = MixedNB(categorical_features=[1,2,3])\n",
    "fit_model(nb_mix, \"Naive Bayes Mixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "trainX = scaler.fit_transform(trainX)\n",
    "testX = scaler.transform(testX)\n",
    "trainX.shape\n",
    "model = Sequential()\n",
    "model.add(Dense(187, input_dim=187, activation='relu'))\n",
    "model.add(Dense(187, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(trainX, trainy, epochs=5, batch_size=10)\n",
    "_, accuracy = model.evaluate(testX, testy)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ensemble Methods**\n",
    "\n",
    "After applying **classification models**, ensemble methods were used, to determine whether it improve the performance of models or not?\n",
    "\n",
    "Several ensemble methods were applied on the **one-hot encoded** dataset one-by-one.\n",
    "\n",
    "These methods include:\n",
    "\n",
    "* Bagging Classifier\n",
    "\n",
    "* Stacking Classifier\n",
    "\n",
    "* Voting Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Bagging** Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:48:42.348961Z",
     "iopub.status.busy": "2022-05-15T13:48:42.348622Z",
     "iopub.status.idle": "2022-05-15T13:48:42.360135Z",
     "shell.execute_reply": "2022-05-15T13:48:42.359254Z",
     "shell.execute_reply.started": "2022-05-15T13:48:42.348928Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=1)\n",
    "reg_bg = BaggingClassifier(base_estimator=GradientBoostingClassifier(max_depth=5, n_estimators=200),\n",
    "                        n_estimators=20, random_state=0)\n",
    "scores = cross_val_score(reg_bg, df_onehot, y, cv=cv)\n",
    "score = format(mean(scores), '.4f')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Stacking** Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:48:42.361989Z",
     "iopub.status.busy": "2022-05-15T13:48:42.361216Z",
     "iopub.status.idle": "2022-05-15T13:48:42.381036Z",
     "shell.execute_reply": "2022-05-15T13:48:42.380289Z",
     "shell.execute_reply.started": "2022-05-15T13:48:42.361952Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=1)#, random_state=1)\n",
    "estimators = [\n",
    "('lr', LogisticRegression()),\n",
    "('dt', DecisionTreeClassifier(max_depth=5)),\n",
    "('rf', RandomForestClassifier(max_depth=20, n_estimators=1000))\n",
    "]\n",
    "\n",
    "reg_sr = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier(max_depth=5, n_estimators=200, random_state=42))\n",
    "scores = cross_val_score(reg_sr, df_onehot, y, cv=cv)\n",
    "score = format(mean(scores), '.4f')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Voting** Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:48:42.383171Z",
     "iopub.status.busy": "2022-05-15T13:48:42.382613Z",
     "iopub.status.idle": "2022-05-15T13:48:42.395313Z",
     "shell.execute_reply": "2022-05-15T13:48:42.394383Z",
     "shell.execute_reply.started": "2022-05-15T13:48:42.383137Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=1)\n",
    "r1 = DecisionTreeClassifier(max_depth=5)\n",
    "r2 = RandomForestClassifier(max_depth=20,n_estimators=1000)\n",
    "r3 = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "\n",
    "reg_vr = VotingClassifier([('dt', r1), ('rf', r2),('gb', r3)])\n",
    "scores = cross_val_score(reg_vr, df_onehot, y, cv=cv)\n",
    "score = format(mean(scores), '.4f')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Pre-Processing** (Filling Missing Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:35:33.019366Z",
     "iopub.status.busy": "2022-05-15T13:35:33.018747Z",
     "iopub.status.idle": "2022-05-15T13:35:35.153718Z",
     "shell.execute_reply": "2022-05-15T13:35:35.152806Z",
     "shell.execute_reply.started": "2022-05-15T13:35:33.019328Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Telecom_customer churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names contains missing values \n",
    "\n",
    "categorical_columns = ['prizm_social_one', 'hnd_webcap', 'ownrent', 'infobase', 'HHstatin', 'dwllsize', 'dwlltype']\n",
    "numeric_columns = ['avg6mou', 'avg6qty', 'avg6rev', 'lor', 'adults', 'income', 'numbcars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Missing categorical columns data with **mode()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in categorical_columns:\n",
    "    mode_value = df[each].mode()\n",
    "    df[each].fillna(mode_value[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling Missing numerical columns data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using **mean()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in numeric_columns:\n",
    "    mean_value = df[each].mean()\n",
    "    df[each].fillna(mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Best three models Results --------------------# \n",
    "\n",
    "# One hot encoding on dataset\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "# X and y Split\n",
    "df_onehot = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df[['churn']]\n",
    "\n",
    "# Train Test Split\n",
    "trainX, testX, trainy, testy = train_test_split(df_onehot, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Logistic Regression\n",
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()),(\"Logistic\", LogisticRegression())])\n",
    "fit_model(pipe_kn, \"Logistic\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(max_depth=20,n_estimators=1000)\n",
    "fit_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using **Median()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in numeric_columns:\n",
    "    median_value = df[each].median()\n",
    "    df[each].fillna(mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Best three models Results --------------------# \n",
    "\n",
    "# One hot encoding on dataset\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "# X and y Split\n",
    "df_onehot = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df[['churn']]\n",
    "\n",
    "# Train Test Split\n",
    "trainX, testX, trainy, testy = train_test_split(df_onehot, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Logistic Regression\n",
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()),(\"Logistic\", LogisticRegression())])\n",
    "fit_model(pipe_kn, \"Logistic\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(max_depth=20,n_estimators=1000)\n",
    "fit_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using **Mode()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in numeric_columns:\n",
    "    mode_value = df[each].median()\n",
    "    df[each].fillna(mode_value[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Best three models Results --------------------# \n",
    "\n",
    "# One hot encoding on dataset\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "# X and y Split\n",
    "df_onehot = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df[['churn']]\n",
    "\n",
    "# Train Test Split\n",
    "trainX, testX, trainy, testy = train_test_split(df_onehot, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Logistic Regression\n",
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()),(\"Logistic\", LogisticRegression())])\n",
    "fit_model(pipe_kn, \"Logistic\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(max_depth=20,n_estimators=1000)\n",
    "fit_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search**\n",
    "Since we do know that **gradient boosting classifier** yield the highest **ROC AUC** value. \n",
    "\n",
    "The last part would be finding the optimal hyperparameter for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Telecom_customer churn.csv')\n",
    "# Dropping NAN values\n",
    "df.dropna(subset=['rev_Mean', 'kid11_15', 'dualband', 'area', 'hnd_price', 'change_mou'], inplace=True)\n",
    "\n",
    "df.drop(['avg6mou', 'avg6qty', 'avg6rev', 'prizm_social_one', 'ownrent', \n",
    "        'lor', 'dwlltype', 'adults', 'infobase', 'numbcars', \n",
    "        'HHstatin', 'dwllsize', 'income', 'hnd_webcap'], \n",
    "         axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df_onehot[['churn']]\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "regRF = GradientBoostingClassifier(max_depth=5, random_state=0)\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'max_features': [2, 3, 4],    \n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300] \n",
    "}\n",
    "grid_search = GridSearchCV(estimator = regRF, param_grid=param_grid, cv = cv, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X, y)\n",
    "best_grid = grid_search.best_estimator_\n",
    "print(best_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Tuning with Cross validation**\n",
    "\n",
    "To estimate the skill of our best model i.e., **Gradient Boosting Classifier** on unseen data. We used **cross validation** for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_onehot = []\n",
    "s_no = []\n",
    "for i in range(0,30):\n",
    "    # prepare the cross-validation procedure\n",
    "    cv = KFold(n_splits=10, random_state=i, shuffle=True)\n",
    "    GB = GradientBoostingClassifier(max_depth=5, random_state=0)\n",
    "    \n",
    "\n",
    "    scores = cross_val_score(GB, df_onehot, y, scoring='roc_auc', cv=cv) \n",
    "    score_onehot.append(mean(scores))\n",
    "    \n",
    "    s_no.append(i)\n",
    "    \n",
    "scores_df = pd.DataFrame(\n",
    "    {'S #': s_no,\n",
    "     'onehot': score_onehot\n",
    "    })\n",
    "scores_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fitted our best model **Gradient Boosting Classifier**, we can now extract the **feature importance**. This is stored in a property called feature_importances_.\n",
    "\n",
    "We sorted them from least to greatest and did remove features one by one which have the least importance and analyze the **ROC AUC** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:48:46.063451Z",
     "iopub.status.busy": "2022-05-15T13:48:46.063154Z",
     "iopub.status.idle": "2022-05-15T13:48:46.127258Z",
     "shell.execute_reply": "2022-05-15T13:48:46.126225Z",
     "shell.execute_reply.started": "2022-05-15T13:48:46.063411Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "y = df_onehot[['churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-15T13:48:46.129330Z",
     "iopub.status.busy": "2022-05-15T13:48:46.129084Z",
     "iopub.status.idle": "2022-05-15T13:56:46.490558Z",
     "shell.execute_reply": "2022-05-15T13:56:46.489658Z",
     "shell.execute_reply.started": "2022-05-15T13:48:46.129300Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(max_depth=5, n_estimators=200, random_state=0)\n",
    "\n",
    "clf.fit(X,y)\n",
    "\n",
    "feature_scores = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Printing features scores\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features having importance vale greater than 0.0069\n",
    "features_gt = feature_scores[feature_scores> 0.0069]\n",
    "\n",
    "# Creating a dataframe of those featurs only\n",
    "X = df_onehot.loc[:, df_onehot.columns != 'churn']\n",
    "X = X[features_gt.index]\n",
    "\n",
    "\n",
    "# Gradient Boosting Classifier (Best Model)\n",
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the results of Feature Importance with Gradient Boosting based on different Threshold values\n",
    "\n",
    "| Threshold Value | Total Features | ROC AUC value |\n",
    "|-----------------|----------------|---------------|\n",
    "| 0               | 155            | 68.14%        |\n",
    "| 0.0001          | 151            | 68.51%        |\n",
    "| 0.001           | 71             | 68.11%        |\n",
    "| 0.01            | 19             | 68.32%        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Winner Model (Gradient Boosting Classifier)**\n",
    "\n",
    "After the implementation of different **classification methods** and **ensemble approaches** with\n",
    "our best models\n",
    "\n",
    "We concluded that our winner model is **Gradient Boosting Classifier** with\n",
    "\n",
    "* max depth = 5\n",
    "* n_estimators = 200\n",
    "* learning_rate = 0.05\n",
    "\n",
    "Generating the **ROC AUC** value around **69%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=200, learning_rate=0.05)\n",
    "fit_model(gb, \"Graident Boosting Classifier\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54ce3980bf0da3a4aa77af90eb2458675cc73ddb313d5ac3f2fab994261f3811"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
